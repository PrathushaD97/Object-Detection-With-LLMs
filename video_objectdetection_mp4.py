# -*- coding: utf-8 -*-
"""video_objectdetection_mp4.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1yKAi1RSmiK1VCZ6_xOvm7N_em64hK_cl
"""

import cv2
import os
from tqdm import tqdm
import requests
import logging
import base64

## HERE IS THE INPUT VIDEO. YOU SET THIS.
input_video_path = 'home_video2.mp4'

# API Key from platform.openai.com. YOU SET THIS TO YOUR API KEY.
api_key=""

## OUTPUT FILES. AUTOMATICALLY SET.
output_framestext_file=input_video_path+"_frametext_out.txt"
output_summary_file=input_video_path+"_summary_out.txt"

## Frames folder. AUTOMATICALLY SET.
output_frames_dir="frames"

# Set up logging
logging.basicConfig(level=logging.DEBUG, filename='video_labeler.log', filemode='w', format='%(asctime)s - %(levelname)s - %(message)s')
console = logging.StreamHandler()
console.setLevel(logging.DEBUG)
formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
console.setFormatter(formatter)
logging.getLogger('').addHandler(console)



## Given an image the following function returns OpenAI's response.

def does_image_contain(image_path, object):
    with requests.Session() as session:
        with open(image_path, "rb") as image_file:
            image_base64 = base64.b64encode(image_file.read()).decode('utf-8')
        headers = {
        "Content-Type": "application/json",
        "Authorization": f"Bearer {api_key}"
        }

        payload = {
          "model": "gpt-4o",
          "messages": [
            {
              "role": "user",
              "content": [
                {
                  "type": "text",
                  "text": "Does this image contain a " + object + "? Answer in a single word 'Yes' or 'No'"
                },
                {
                  "type": "image_url",
                  "image_url": {
                    "url": f"data:image/jpeg;base64,{image_base64}"
                  }
                }
              ]
            }
          ],
          "max_tokens": 300
        }

        response = session.post("https://api.openai.com/v1/chat/completions",
                                    headers=headers, json=payload)

        if response.status_code == 200:
            description = response.json()['choices'][0]['message']['content'].strip()
            return description
        else:
            logging.error(f"OpenAI API error: {response.status_code}, {response.text}")
            return "Error in getting description"

# Given the output_framestext_file as the input parameter the function returns the summary text for all the frames.


def where_in_image(image_path, object):
    with requests.Session() as session:
    # Convert image to base64
        with open(image_path, "rb") as image_file:
            image_base64 = base64.b64encode(image_file.read()).decode('utf-8')
        headers = {
        "Content-Type": "application/json",
        "Authorization": f"Bearer {api_key}"
        }

        payload = {
          "model": "gpt-4o",
          "messages": [
            {
              "role": "user",
              "content": [
                {
                  "type": "text",
                  "text": "Where in the image is the " + object + " located? Briefly describe its position and immediate surroundings"
                },
                {
                  "type": "image_url",
                  "image_url": {
                    "url": f"data:image/jpeg;base64,{image_base64}"
                  }
                }
              ]
            }
          ],
          "max_tokens": 300
        }

        response = session.post("https://api.openai.com/v1/chat/completions",
                                    headers=headers, json=payload)

        if response.status_code == 200:
            description = response.json()['choices'][0]['message']['content'].strip()
            return description
        else:
            logging.error(f"OpenAI API error: {response.status_code}, {response.text}")
            return "Error in getting description"

# Load video
cap = cv2.VideoCapture(input_video_path)
if not cap.isOpened():
    raise Exception(
        f"Error: Could not open input video file: {input_video_path}")

width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
fps = cap.get(cv2.CAP_PROP_FPS)
frame_interval = int(fps * 0.5)

# Create an output directory for frames
os.makedirs(output_frames_dir, exist_ok=True)

from google.colab.patches import cv2_imshow

object_to_find = 'keys'

# Open the text file to write detected objects.
# Also, save the frames that are sent to OpenAI
with open(output_framestext_file,
          "w") as file, tqdm(total=int(cap.get(cv2.CAP_PROP_FRAME_COUNT)),
         desc="Processing frames") as pbar:
    frame_count = 0
    while cap.isOpened():
        ret, frame = cap.read()
        if ret:
          if frame_count % frame_interval == 0:
            frame_path = os.path.join(output_frames_dir, f"frame_{frame_count}.jpg")
            cv2.imwrite(frame_path, frame)

            if does_image_contain(frame_path, object_to_find) == 'Yes':
              print('Found the ' + object_to_find)
              cv2_imshow(frame)
              print(where_in_image(frame_path, object_to_find))
              break
            else:
              print('Could not find ' + object_to_find)
        frame_count+=1

